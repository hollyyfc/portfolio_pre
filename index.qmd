---
title: "`lme4u`: An R Package"
subtitle: "Interpretation and Diagnostics for `lme4`"
author: "Holly Cui"
format: revealjs
editor: visual
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(lme4u)
library(lme4)
library(tidyverse)
```

# Introduction

## Mixed-Effects Modeling

Analyze data where observations are structured within groups, with fixed and random effects. 

<br>

```{r, eval=FALSE, echo=TRUE, class = "fragment"}
lm(Score ~ Instruction, data = mydata)
```

<br>

```{r, eval=FALSE, echo=TRUE, class = "fragment"}
lme4::lmer(Score ~ Instruction + (1 | School), data = mydata)
```

<br>

```{r, eval=FALSE, echo=TRUE, class = "fragment"}
lme4::glmer(Score ~ Instruction + (1 | School), family = binomial(link = "logit"), data = mydata)
```

## Why `lme4u`?

- **Interprets mixed-effects models**: provides user-friendly interpretations for fixed and random effects.
- **Assumption diagnostics**: automates residual analysis and visualizations for heterogeneity checks.
- **Hypothesis Testing**: facilitates hypothesis testing specific to mixed structure.
- **Example Data**: offers a cleaned dataset curated for the purpose of mixed-effect modeling.

# Data

A brand new dataset for mixed-effect modeling

## `star`

A cleaned subset of the Project [STAR](https://search.r-project.org/CRAN/refmans/AER/html/STAR.html) (student-teacher achievement ratio) dataset.

```{r, echo=TRUE}
glimpse(star)
```

# Objective and Modeling

## Question we ask ...

::: {style="font-size: 70%;"}
> _Does class size impact students' math test performance, and how does this effect vary across schools?_

:::

$$
\scriptsize \text{MathScore}_{ij} = \beta_0 + \beta_1(\text{ClassType}_{ij}) + b_{0j} + b_{1j}(\text{ClassType}_{ij}) + \epsilon_{ij}
$$

- $\tiny i \text{ indexes students},\ j \text{ indexes schools.}$
- $\tiny \beta_0, \beta_1 \text{ are fixed intercept and slope.}$
- $\tiny b_{0j} \sim N(0, \tau^2_{b0}) \text{ is a random intercept for each school.}$
- $\tiny b_{1j} \sim N(0, \tau^2_{b1}) \text{ is a random slope for ClassType in each school.}$
- $\tiny \epsilon_{ij} \sim N(0, \sigma^2_{\epsilon}) \text{ is the residual error (within-school heterogeneity).}$

## Modeling

In R using `lme4`, this would be implemented as:

```{r, echo=TRUE}
lme4::lmer(math ~ cltype + (cltype | school_id), data = star)
```

# Package Features

<br>

```{r, eval=FALSE, echo=TRUE}
# Installation 
devtools::install_github("hollyyfc/lme4u")
library(lme4u)
```

 
## <span style="font-size: 55px;">`explain_lmer(model, details)`</span>

<br>

::: {style="border: 2px solid black; padding: 10px; background-color: #ececec; font-size: 30px"}
✨ A core component of the package, which provides structured, user-friendly interpretations of linear mixed-effects models fitted using `lme4::lmer()`; supports three levels of interpretation via the `details` argument.
:::

<br>

To use this function, first do:
```{r, message=FALSE, echo=TRUE}
model <- lme4::lmer(math ~ cltype + math_old + (cltype | school_id), data = star)
```

## <span style="font-size: 55px;">"general"</span>

::: {style="font-size: 30px"}
Default for `details` argument, which provides a high-level summary of the model, describing its structure, fixed effects, and random effects in a concise yet informative manner:
:::

<div style="margin-top: 20px;"></div>

```{r, echo=TRUE}
explain_lmer(model)
```

## <span style="font-size: 55px;">"fixed"</span>

::: {style="font-size: 30px"}
A detailed explanation of the fixed effects, including interpretations of individual estimates and their implications within the model context:
:::

<div style="margin-top: 20px;"></div>

```{r, echo=TRUE}
fix <- explain_lmer(model, "fixed")
```

---

```{r, echo=TRUE}
attr(fix, "fixed_details")
```

::: callout-tip
## Note

If interaction is fitted to the model, the function is also able to detect and adjust the interpretation based on the interaction structure. 
:::

## <span style="font-size: 55px;">"random"</span>

::: {style="font-size: 30px"}
A detailed explanation of random intercepts, slopes, and variance components to highlight across-group variability:
:::

<div style="margin-top: 20px;"></div>

```{r, echo=TRUE, message=FALSE}
# Nested-grouping example
model_nest <- lmer(math ~ cltype + math_old + (1|system_id/school_id), data = star)
random_nest <- explain_lmer(model_nest, "random")
```

---

```{r, echo=TRUE}
attr(random_nest, "var_details")
```

::: callout-tip
## Note

The "random" part is also capable of detecting uncorrelated random effects and multiple (>= 3) random terms.
:::


## <span style="font-size: 42px;">`lrt_lmer(model, target, type, data, ...)`</span>

<br>

::: {style="border: 2px solid black; padding: 10px; background-color: #ececec; font-size: 30px"}
✨ Hypothesis testing on a selected fixed or random effect in a fitted lmer model via likelihood ratio test. Currently only support models fitted with `lme4::lmer()` that contain fixed effects and a single random structure.
:::

<br>

::: {style="font-size: 35px"}
- $H_0$: The selected variable has no valid effect (i.e., its coefficient is 0 for fixed effects, or its variance component is 0 for random effects).

- $H_1$: The selected variable has an effect (i.e., not 0).
:::

## How-to under HLM {.smaller}

::: {style="font-size: 26px; padding: 10px; margin: auto;"}

- Cannot ignore grouping: observations not independent across groups
- Control for all desired covariates properly
- Ensure hierarchical linear models (HLM) instead of `lm` OLS or ANOVA
:::

<div style="width: 100%; margin: auto; transform: scale(1.3); text-align: center;">
```{mermaid}
%%| fig-width: 12
flowchart LR
  A[Macro Variables] --> B[Within-Group Correlation]
  B --> A
```
</div>


::: columns
::: {.column width="50%"}
::: {style="font-size: 26px;"}

- Construct a reduced model based on the null hypothesis (by removing the target variable)
- Compare the full ($M_1$) vs. reduced ($M_0$) model fits 
- Use the likelihood ratio test (LRT): $$\lambda(\textbf{y}) = 2 \log \left( \frac{L_1}{L_0} \right) \sim \chi^2$$
:::
:::

::: {.column width="50%"}
::: {style="font-size: 24px; background-color: #f5f5f5; border-left: 4px solid #999; padding: 10px;"}

✅ For each variable type (fixed or random), decide:

- the **reduced model** under $H_0$
- the appropriate **null distribution** for the test statistic $\lambda(\textbf{y})$
- reject $H_0$ if $\lambda_{obs}$ is larger than the critical value
:::
:::

:::

##  <span style="font-size: 45px;">Fixed Effect</span>


<div style="font-size: 25px;">
::: panel-tabset
## Math

**$M_1$ (full model)**: with fixed effect of $x_{i,j}$
$$
\begin{aligned}
y_{i,j} &= \beta_0 + \beta_1x_{i,j} + a_j + \epsilon_{i,j} \\
a_j &\sim N(0,\tau^2)
\end{aligned}
$$
**$M_0$ (reduced model)**: without fixed effect of $x_{i,j}$
$$
\begin{aligned}
y_{i,j} &= \beta_0 + a_j + \epsilon_{i,j} \\
a_j &\sim N(0,\tau^2)
\end{aligned}
$$

**Null distribution**: where the change in the number of parameters is $d=1$
$$\lambda(\textbf{y}) \sim \chi^2_d$$ 

## Code

<div style="font-size: 30px;">

```{r, echo=TRUE}
fix_result <- lrt_lmer(model, target = "math_old", type = "fixed", data = star)
```


```{r, echo=TRUE}
fix_result
```

</div>

:::
</div>


##  <span style="font-size: 45px;">Random Intercept</span>

<div style="font-size: 20px;">
::: panel-tabset
## Math

**$M_1$ (full model)**: with random intercept (within-group correlation) $\tau^2$
$$
\mathbf{y_j} = \mathbf{X_j\beta} + \epsilon_{j},\ 
\text{Cov} \left[
\begin{pmatrix}
\epsilon_{1,j} \\
\vdots \\
\epsilon_{n,j}
\end{pmatrix}
\right]
=
\begin{pmatrix}
\sigma^2 + \tau^2 & \tau^2 & \cdots & \tau^2 \\
\tau^2 & \sigma^2 + \tau^2 & \cdots & \tau^2 \\
\vdots & \vdots & \ddots & \vdots \\
\tau^2 & \tau^2 & \cdots & \sigma^2 + \tau^2
\end{pmatrix}
$$
**$M_0$ (reduced model)**: without random intercept $\tau^2$
$$
\mathbf{y_j} = \mathbf{X_j\beta} + \epsilon_{j},\ 
\text{Cov} \left[
\begin{pmatrix}
\epsilon_{1,j} \\
\vdots \\
\epsilon_{n,j}
\end{pmatrix}
\right]
=
\begin{pmatrix}
\sigma^2 & 0 & \cdots & 0 \\
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2
\end{pmatrix}
$$

**(Asymptotic) Null distribution**: $\lambda(\mathbf{y})$ has a mixture distribution under $M_0$ ($d=1$)
$$
\lambda(\mathbf{y}) =
\begin{cases}
X_0 = 0 & \text{with probability } 1/2 \\
X_1 \sim \chi^2_1 & \text{with probability } 1/2
\end{cases}
$$

## Code

<div style="font-size: 27px;">

```{r, echo=TRUE}
random_intercept <- lrt_lmer(model, target = "school_id", type = "random", data = star)
```

::: {.callout-note title="Calculating p-value under mixture null" appearance="minimal"}
Recall that a p-value is the probability under the null distribution of getting a test statistic equal to or larger than the observed test statistic. 

**Folklore**: _"The p-value for testing ... the random intercept variance is half this [$\chi^2_1$] tail value"_ (only if $\lambda_{obs} \neq 0$). Thus:
$$
\text{Pr}(\lambda(\mathbf{y}) \geq \lambda_{obs}) = \frac{1}{2} \text{Pr}(\chi^2_1 \geq \lambda_{obs}),\ \lambda_{obs}>0
$$
In R, we translate to: 

<div style="text-align: center;">
  <code>0.5 * (1 - pchisq(lambda, 1))</code>
</div>
:::

</div>

:::
</div>



##  <span style="font-size: 45px;">Random Slope</span>

<div style="font-size: 20px;">
::: panel-tabset

## Math

$$
y_{i,j} = \mathbf{\beta^\intercal x_{i,j}} + \mathbf{a_j^\intercal z_{i,j}} + \epsilon_{i,j}
$$
If $\mathbf{a_j} \in \mathbb{R}^p$, then
$$
\text{Cov}[\mathbf{a}_j] = \Psi =
\begin{pmatrix}
\psi_1^2 & \psi_{12} & \cdots & \psi_{1p} \\
\psi_{21} & \psi_2^2 & \cdots & \psi_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
\psi_{p1} & \psi_{p2} & \cdots & \psi_p^2
\end{pmatrix}
$$
**$M_1$ (full model)**: \ with $\psi^2_p \neq 0$ and covariances $\psi_{pk} \neq 0$

**$M_0$ (reduced model)**: \ $\psi^2_p=0$ and covariances $\psi_{pk}=0$

**(Asymptotic) Null distribution**: \ $\lambda(\mathbf{y})$ has a mixture distribution under $M_0$ ($d=p$)
$$
\lambda(\mathbf{y}) =
\begin{cases}
X_{p-1} \sim \chi^2_{p-1} & \text{with probability } 1/2 \\
X_p \sim \chi^2_p & \text{with probability } 1/2
\end{cases}
$$

## Code
<div style="font-size: 28px;">
```{r, echo=TRUE}
model <- lmer(math ~ cltype + math_old + (math_old|school_id), data = star) # change model later?
random_slope <- lrt_lmer(model, target = "math_old", type = "random", data = star)
```

::: {.callout-note title="Calculating p-value under mixture null" appearance="minimal"}

Similar to the previous calculation of p-value, we have:
$$
\text{Pr}(\lambda(\mathbf{y}) \geq \lambda_{obs}) = \frac{1}{2} \left( \text{Pr}(\chi^2_{p-1} \geq \lambda_{obs}) + \text{Pr}(\chi^2_p \geq \lambda_{obs}) \right)
$$

In R, we translate to: 

<div style="text-align: center; justify-content: center;">
  <code>0.5 * ((1 - pchisq(lambda, p-1)) + (1 - pchisq(lambda, p)))</code>
</div>
:::

</div>

:::
</div>










# Future Work


## (TBD)

## 

::: {style="text-align: center;"}
```{=html}
<iframe width="1800" height="680" src="https://hollyyfc.github.io/lme4u/" frameborder="0" style="background:white;"></iframe>
```
:::






